{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMxWrX0I0zL7"
      },
      "source": [
        "# Let's load the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-O2H8vqMq-Y"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EfNY0x_L_Eu"
      },
      "source": [
        "# !pip install mne\n",
        "# !pip install h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8mE4IlnMHaR"
      },
      "source": [
        "import matplotlib\n",
        "import pathlib \n",
        "#import mne\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import os\n",
        "# from os import listdir\n",
        "# from os.path import isfile, join\n",
        "# from tqdm import tqdm \n",
        "# import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8b4njvVMJXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab08ef2e-a358-417e-e768-940447d137fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFeXjqg1i9SG"
      },
      "source": [
        "## Importing EEG and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xABlLpxnMw6q"
      },
      "source": [
        "df=pd.read_excel('/content/drive/My Drive/deap_dataset.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxe0jaqyUgs4"
      },
      "source": [
        "part1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP1VetUBMyhQ"
      },
      "source": [
        "eeg=[]\n",
        "\n",
        "raw_path = \"/content/drive/My Drive/DEAP-Orginal/part1/\" '''sample 1 to 23'''\n",
        "for j in tqdm(os.listdir(raw_path)):\n",
        "    raw = mne.io.read_raw(os.path.join(raw_path,j))\n",
        "    raw=raw.copy().pick_types(meg=False, eeg=True, eog=False, exclude=['F3','F7','FC5','FC1','CP5','CP1','P3','P7','PO3','AF4','Fz','F4','FC6',\n",
        "                                                                       'FC2','Cz','C4','CP6','CP2','P8',\n",
        "                                                                       'EXG1','EXG2','EXG3', 'EXG4', 'EXG5', 'EXG6','EXG7','EXG8',\n",
        "                                                                       'GSR1','GSR2','Erg1','Erg2','Resp','Plet','Temp','Status'])\n",
        "    raw.set_montage('standard_1020')\n",
        "    raw.resample(128)\n",
        "    #raw.load_data()\n",
        "    raw.filter(l_freq=3, h_freq=47)\n",
        "    eeg.append(raw)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELJDIvddUjBQ"
      },
      "source": [
        "part2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQW80b1kUkZ6"
      },
      "source": [
        "raw_path = \"/content/drive/My Drive/DEAP-Orginal/part2/\" '''sample 24 to 28'''\n",
        "for j in tqdm(os.listdir(raw_path)):\n",
        "    raw = mne.io.read_raw_edf(os.path.join(raw_path,j), preload=True, stim_channel=\"\")\n",
        "    mne.rename_channels(raw.info,mapping={\"\":\"Status\"})\n",
        "    raw=raw.copy().pick_types(meg=False, eeg=True, eog=False, exclude=['F3','F7','FC5','FC1','CP5','CP1','P3','P7','PO3','AF4','Fz','F4','FC6',\n",
        "                                                                       'FC2','Cz','C4','CP6','CP2','P8',\n",
        "                                                                       'EXG1','EXG2','EXG3', 'EXG4', 'EXG5', 'EXG6','EXG7','EXG8',\n",
        "                                                                       'GSR1','GSR2','Erg1','Erg2','Resp','Plet','Temp','Status'])\n",
        "    raw.set_montage('standard_1020')\n",
        "    raw.resample(128)\n",
        "    #raw.load_data()\n",
        "    raw.filter(l_freq=3, h_freq=47)\n",
        "    eeg.append(raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPs5bBi8UyKo"
      },
      "source": [
        "part3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cxxfI4YUxlh"
      },
      "source": [
        "raw_path = \"/content/drive/My Drive/DEAP-Orginal/part3/\"  '''sample 29 to 32'''\n",
        "for j in tqdm(os.listdir(raw_path)):\n",
        "    raw = mne.io.read_raw_edf(os.path.join(raw_path,j), preload=True, stim_channel=\"-1\")\n",
        "    mne.rename_channels(raw.info,mapping={\"-1\":\"Status\"})\n",
        "    raw=raw.copy().pick_types(meg=False, eeg=True, eog=False, exclude=['F3','F7','FC5','FC1','CP5','CP1','P3','P7','PO3','AF4','Fz','F4','FC6',\n",
        "                                                                       'FC2','Cz','C4','CP6','CP2','P8',\n",
        "                                                                       'EXG1','EXG2','EXG3', 'EXG4', 'EXG5', 'EXG6','EXG7','EXG8',\n",
        "                                                                       'GSR1','GSR2','Erg1','Erg2','Resp','Plet','Temp','-0','Status'])\n",
        "    raw.set_montage('standard_1020')\n",
        "    raw.resample(128)\n",
        "    #raw.load_data()\n",
        "    raw.filter(l_freq=3, h_freq=47)\n",
        "    eeg.append(raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhEBhB5YjH4m"
      },
      "source": [
        "## Cropping EEG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SfEBHoFXhJO"
      },
      "source": [
        "FIRST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMarrXWjXULf"
      },
      "source": [
        "X=[]\n",
        "Y_1=[]\n",
        "Y_2=[]\n",
        "Y_3=[]\n",
        "for j in range(len(eeg)):\n",
        "  for i in range(40):\n",
        "    n=df['start'][i+j*40]\n",
        "    m=df['finish'][i+j*40]\n",
        "    raw_cropped = eeg[j].copy().crop(tmin=n,tmax=m) #s\n",
        "    X.append(raw_cropped.get_data())\n",
        "    Y_1.append(df['label1'][i+j*40])\n",
        "    Y_2.append(df['label2'][i+j*40])\n",
        "    Y_3.append(df['label3'][i+j*40])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwx6obu6Xja5"
      },
      "source": [
        "SECOND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCuQrzRzVTMg"
      },
      "source": [
        "X=[]\n",
        "y=[]\n",
        "for k in range(1,33):\n",
        "  for i in range(40*(k-1),40*k):\n",
        "    n=df['start'][i]\n",
        "    m=df['finish'][i]\n",
        "    raw_cropped = eeg[k-1].copy().crop(tmin=n,tmax=m)\n",
        "    X.append(raw_cropped.get_data())\n",
        "    y.append(df['label1'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e8XnVvC2N-E"
      },
      "source": [
        "## Save and Load X,Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToQyRnMVZHta"
      },
      "source": [
        "Convert to Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZEPmvl-PFMa"
      },
      "source": [
        "X=np.asarray(X)\n",
        "Y=np.asarray(Y_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z148wMjYYOS2"
      },
      "source": [
        "Y2=np.asarray(Y_2)\n",
        "Y3=np.asarray(Y_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH-1FmjMX5M6"
      },
      "source": [
        "Save the Arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhYVwPh4X7iY"
      },
      "source": [
        "np.savez(\"/content/drive/My Drive/DEAP-Orginal/xy/X_eeg\",X)\n",
        "np.savez(\"/content/drive/My Drive/DEAP-Orginal/xy/Y_1\",Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro7nCitOYaGn"
      },
      "source": [
        "np.savez(\"/content/drive/My Drive/DEAP-Orginal/xy/Y_2\",Y2)\n",
        "np.savez(\"/content/drive/My Drive/DEAP-Orginal/xy/Y_3\",Y3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DahlWDECeLTn"
      },
      "source": [
        "Load the Arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nn7MYMeN0s"
      },
      "source": [
        "#load X data\n",
        "a = np.load(\"/content/drive/My Drive/DEAP-Orginal/xy/X_eeg.npz\") #make sure you use the .npz!\n",
        "X = a['arr_0']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_r_W_ZeeUGw"
      },
      "source": [
        "#load Y data '''Y_1 : 4 categories, Y_2: 10 categories, Y_3: 2 categories'''\n",
        "b = np.load(\"/content/drive/My Drive/DEAP-Orginal/xy/Y_3.npz\") #make sure you use the .npz!\n",
        "Y = b['arr_0']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##One-hot label"
      ],
      "metadata": {
        "id": "JjRxxurqrBoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "dbPEshChrAg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=Y\n",
        "values = array(data)\n",
        "print(values)"
      ],
      "metadata": {
        "id": "UE0BHk8uq9b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(integer_encoded)"
      ],
      "metadata": {
        "id": "Ob83bNsfrOGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)"
      ],
      "metadata": {
        "id": "cKT6FfkPrPdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
        "print(inverted)"
      ],
      "metadata": {
        "id": "KaIIm7iwrQx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXmYFHhwrSSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQDro4lbaBlo"
      },
      "source": [
        "# Building Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB9vwKZlbKsB"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4s49X6CPenx"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from random import shuffle\n",
        "from keras import models\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling1D,MaxPooling2D, ConvLSTM2D,Conv1D, AveragePooling2D,LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "#from keras.optimizers import Adam, SGD \n",
        "import time\n",
        "from keras import regularizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers import Activation, Dropout, BatchNormalization, Flatten, Dense, AvgPool2D,UpSampling2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "from itertools import cycle\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ksI6ZW6rXpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWFklzSean7Z"
      },
      "source": [
        "## Create Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3dWvTMRax4f"
      },
      "source": [
        "### Model1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EsYOWNkPu4U",
        "outputId": "be80e421-2a61-4a29-bbc5-f5543d2aa2ac"
      },
      "source": [
        "model1 = keras.models.Sequential()\n",
        "\n",
        "model1.add(Conv1D(64, kernel_size=3,padding='same', activation='relu',input_shape = (13, 7681)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model1.add(Conv1D(32, kernel_size=3,padding='same', activation='relu'))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model1.add(Flatten())\n",
        "# model1.add(Dense(64, activation=\"relu\"))\n",
        "model1.add(Dense(32, activation=\"relu\"))\n",
        "model1.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 13, 64)            1474816   \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 13, 64)           256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 6, 64)            0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 6, 32)             6176      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 6, 32)            128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 3, 32)            0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 96)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                3104      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,484,546\n",
            "Trainable params: 1,484,354\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaa0Xf_9Pu6w"
      },
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
        "history = model1.fit(X_train, y_train_onehot, epochs=20, batch_size =128 , validation_data=(X_test,y_test_onehot))\n",
        "#cweights_bykeras_dict\n",
        "#model1_json = model1.to_json()\n",
        "#with open(\"model1.json\", \"w\") as json_file:\n",
        "    #json_file.write(model1_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZraJYagAa3Cg"
      },
      "source": [
        "### Model2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cU8gRkC6SjC"
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Conv1D(128, kernel_size=7, padding='same', activation='relu', input_shape=(13, 7681)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model2.add(Conv1D(64, kernel_size=5, padding='same', activation='relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(MaxPooling1D(pool_size=1))\n",
        "\n",
        "model2.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(MaxPooling1D(pool_size=1))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(64, activation='relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(2, activation='softmax'))\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmNK_drz7lGr"
      },
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
        "history = model2.fit(X_train, y_train_onehot, epochs=900, batch_size =256 , validation_data=(X_test,y_test_onehot))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjNJ-XSiZ_Lw"
      },
      "source": [
        "### Model3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.regularizers.l1(0.01)\n",
        "keras.regularizers.l2(0.01)\n",
        "keras.regularizers.l1_l2(l1=0.01, l2=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWttQk4Pqx0x",
        "outputId": "f0be165b-f43a-42fc-8a35-88ad85ae49a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.regularizers.L1L2 at 0x7f83ebb920d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.regularizers import l2,l1"
      ],
      "metadata": {
        "id": "gPBLIozZrHHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDtlFk52ClnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f57c56df-f03e-44dc-9bfb-7c2272821e68"
      },
      "source": [
        "# make a sequential model\n",
        "model3 = keras.models.Sequential()\n",
        "\n",
        "# add 1-layer cnn\n",
        "model3.add(Conv1D(128, kernel_size=8, padding='same',activation='relu' ,strides=1,input_shape=(13, 7681))) #(13, 7681)\n",
        "# model3.add(Dropout(0.25))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "\n",
        "# add 3-layer lstm\n",
        "model3.add(LSTM(128, return_sequences=True, stateful=False))\n",
        "model3.add(Dropout(0.25))\n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "model3.add(LSTM(64, return_sequences=True, stateful=False))\n",
        "model3.add(Dropout(0.25))\n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "# model3.add(LSTM(32, return_sequences=True, stateful=False))\n",
        "# model3.add(Dropout(0.25))\n",
        "# model3.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model3.add(Flatten())\n",
        "# model3.add(Dense(64, activation=\"relu\")) #,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "model3.add(Dense(2, activation='softmax'))\n",
        "model3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 13, 128)           7865472   \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 13, 128)          512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 6, 128)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 6, 128)            131584    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 6, 128)            0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 6, 128)           512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 6, 64)             49408     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 6, 64)             0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 6, 64)            256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 384)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 770       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,048,514\n",
            "Trainable params: 8,047,874\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a sequential model\n",
        "model3 = keras.models.Sequential()\n",
        "\n",
        "# add 1-layer cnn\n",
        "model3.add(Conv1D(128, kernel_size=7, padding='same',activation='relu' ,strides=1,input_shape=(7681,13))) #(13, 7681)\n",
        "model3.add(Dropout(0.15))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# model3.add(Conv1D(128, kernel_size=5, padding='same',activation='relu' ,strides=1))\n",
        "# model3.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# model3.add(Conv1D(64, kernel_size=3, padding='same',activation='relu' ,strides=1))\n",
        "# model3.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# add 3-layer lstm\n",
        "model3.add(LSTM(64, return_sequences=True, stateful=False))\n",
        "model3.add(Dropout(0.25))\n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "model3.add(LSTM(64, return_sequences=True, stateful=False))\n",
        "model3.add(Dropout(0.25))\n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "# model3.add(LSTM(32, return_sequences=True, stateful=False))\n",
        "# model3.add(Dropout(0.25))\n",
        "# model3.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model3.add(Flatten())\n",
        "# model3.add(Dense(128, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\")) #,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
        "model3.add(Dense(4, activation='softmax'))\n",
        "model3.summary()"
      ],
      "metadata": {
        "id": "H9IwvhVcfSDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1LpAAXzl9gN"
      },
      "source": [
        "### Model4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFjeV73ll_Tu"
      },
      "source": [
        "model4 = Sequential()\n",
        "\n",
        "model4.add(LSTM(256, input_shape = (13, 7681),return_sequences=True))\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model4.add(LSTM(128,activation=\"relu\",return_sequences=True))\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.3))\n",
        "\n",
        "model4.add(LSTM(64,activation=\"relu\",return_sequences=True))\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model4.add(LSTM(32,activation=\"relu\"))\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "model4.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "model4.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB55YWWv9RnK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilqWADd8CsMI"
      },
      "source": [
        "## Fit the model on data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJv0_TXtgEeg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc157f10-17f8-476a-a742-1bc94ca5049e"
      },
      "source": [
        "#sample_size=[0.1,0.15,0.2,0.25,0.3,0.33,0.35,0.4,0.45,0.5]\n",
        "lst_test=[]\n",
        "lst_train=[]\n",
        "each_train_acc=[]\n",
        "each_train_loss=[]\n",
        "loss_lst_train=[]\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "opt1 = keras.optimizers.Adam(learning_rate=0.002)\n",
        "opt2 = keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "opt3 = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00, amsgrad=False)\n",
        "opt4 = keras.optimizers.RMSprop(learning_rate=0.003)\n",
        "cv=range(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line = ['solid', (0, (5, 5)), (0, (3, 5, 1, 5)), (0, (3, 1, 1, 1)), (0, (1, 1)), (0, (5, 10)), (0, (5, 1)), (0, (3, 5, 1, 5, 1, 5)), (0, (3, 1, 1, 1, 1, 1)), (0, (3, 10, 1, 10, 1, 10)), (0, (1, 10)), 'dashdot']"
      ],
      "metadata": {
        "id": "Fe6jlvQvuCR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9mfJjMGtH30"
      },
      "source": [
        "start=time.time()\n",
        "for j in cv:\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split (X,onehot_encoded , test_size=0.33,shuffle=True)  #onehot_encoded\n",
        "    # y_test_onehot = keras.utils.to_categorical(y_test,num_classes=5)\n",
        "    # y_train_onehot = keras.utils.to_categorical(y_train,num_classes=5)\n",
        "    model1.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])  # binary_crossentropy #categorical_crossentropy\n",
        "    history = model1.fit(X_train, y_train, epochs=100, batch_size =256,validation_data=(X_test,y_test)) \n",
        "    #class_weight=cweights_bykeras_dict, callbacks = [EarlyStopping(monitor='val_acc', patience=2)]\n",
        "\n",
        "    score_test = model1.evaluate(X_test,y_test, verbose=1)\n",
        "    lst_test.append(score_test)\n",
        "    \n",
        "    \n",
        "    score_train=(history.history['accuracy'])\n",
        "    each_train_acc.append(score_train)\n",
        "    mean_acc=np.mean(score_train)\n",
        "    lst_train.append(mean_acc)\n",
        "\n",
        "    loss_train=(history.history['loss'])\n",
        "    each_train_loss.append(loss_train)\n",
        "    mean_loss=np.mean(loss_train)\n",
        "    loss_lst_train.append(mean_loss)\n",
        "\n",
        "    plt.plot(history.history['accuracy'], color='black',linestyle=line[0],linewidth=3)\n",
        "    plt.plot(history.history['val_accuracy'], color='gray',linestyle=line[4],linewidth=3)\n",
        "    plt.ylim([0.1, 1.1])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Train', 'Test', ''], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'], color='black',linestyle=line[0],linewidth=3)\n",
        "    plt.plot(history.history['val_loss'], color='gray',linestyle=line[3],linewidth=3)\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Train', 'Test',''], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "stop=time.time()\n",
        "print(stop-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " score_test = model1.evaluate(X_test,y_test, verbose=2)"
      ],
      "metadata": {
        "id": "votLpquIrLPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YUL3Qa1hsDDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-6Ie0MuU3jn"
      },
      "source": [
        "## Mean of Test set Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2hiLigHSdrr"
      },
      "source": [
        "test_mean_acc=[]\n",
        "for i in range(0,len(lst_test)):\n",
        "  test_mean_acc.append(lst_test[i][1])\n",
        "\n",
        "test_mean_acc_model=np.mean([test_mean_acc])\n",
        "print(test_mean_acc_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEhVE8H4C3bV"
      },
      "source": [
        "train_mean_acc=[]\n",
        "for i in range(0,len(lst_train)):\n",
        "  train_mean_acc.append(lst_train[i])\n",
        "\n",
        "train_mean_acc_model=np.mean([train_mean_acc])\n",
        "print(train_mean_acc_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFoq9GfIVUbA"
      },
      "source": [
        "## Mean of Test set Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-kJoK9jVcdG"
      },
      "source": [
        "test_mean_loss=[]\n",
        "for i in range(len(lst_test)):\n",
        "  test_mean_loss.append(lst_test[i][0])\n",
        "\n",
        "test_mean_loss_model=np.mean([test_mean_loss])\n",
        "print(test_mean_loss_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-iNiHGGx4zn"
      },
      "source": [
        "test_mean_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aSPTIEZYVVH"
      },
      "source": [
        "train_mean_loss=[]\n",
        "for i in range(len(loss_lst_train)):\n",
        "  train_mean_loss.append(loss_lst_train[i])\n",
        "\n",
        "train_mean_loss_model9=np.mean([train_mean_loss])\n",
        "print(train_mean_loss_model9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8wMlUdDztG6"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77celNSYzUo7"
      },
      "source": [
        "print(history.history.keys())\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'],color='green')\n",
        "plt.ylim([0.4, 1.1])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Test'], loc='best',fontsize='medium')\n",
        "plt.show()\n",
        "\n",
        "# from matplotlib import pyplot\n",
        "# plt.plot(history.history['accuracy'])\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9a2R1PazuUa"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBX_insWzXjc"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'],color='green')\n",
        "plt.ylim([-0.1,1])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Test'],loc='best',fontsize='medium')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFmvLi4m0QYG"
      },
      "source": [
        "Test plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgwvGAaV0A-Z"
      },
      "source": [
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Validation')\n",
        "plt.ylabel('Validation accuracy')\n",
        "plt.xlabel('Validation loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}